---
title: ICML 2017
layout: post
tags: 
local: 2017-08-06-ICML-2017
---

This is the first ever academic conference that I'm attending. I paid for it out
of my own pocket because I wanted to experience first hand what an academic
conference is like.

# Day 1 - Tutorials

There were 9 different tutorials at 3 different locations today. You could only
choose 3 to go for. Selecting tutorials are a tough choice. Going for all would
be optimal. In fact, going for every single talk would be great! However, that
would mean that the conference would span a month or more. Well, I guess it's
structured this way because not everyone can master everything. There will
come a point in time where it's beyond the capacity of a human to be well versed
in so many domains. 

## Interpretable Machine Learning

I was expecting more of methods that exactly show us how to interpret a deep
learning model. Some introductions to papers in this area were made but I've
seen them previously. Other than that, I got the idea that it's more of
formalizing exactly what interpretability is. I think this is a nascent idea and 
definitely very important in the field of Machine Learning (hardcore theory
people definitely think otherwise). I'm always of the idea that diversity of
ideas are important and everyone will play a small part in advancing the field
and everything will somehow converge eventually. Interpretability is important
for practical users; formal proofs are important for academic rigour and
potentially practical use (if they exist).

## Recent Advances in Stochastic Convex and Non-Convex Optimization

Allen Zhu is an incredible speaker. He's got a really impressive CV and is
extremely technical but can bring the talk down to a level where most can
understand (I think?). I don't claim to understand everything fully, but I
thought I could at least follow most of it.

I was introduced to primal, primal-dual, and dual forms of optimization
functions. This was totally new to me. I have no idea how these forms work and I
should really look into that if I intend to get a stronger foundation in
optimization. I was also introduced to the ideas of coordinate descent and
mirror descent. I think all these theories are extremely interesting (like how
coordinate descent guarantees a correct step). 

What struck me the most was when he said that momentum fails for SGD and SVRG.
He mentioned this in the context of convex optimization and offline learning.
Empirically, they work very well on almost all deep learning problems (which is
non-convex). So how can he say that it fails? My interpretation would be that
he's looking at it from a very strict optimization stand point of reaching
global minima. In that perspective, momentum is definitely worse as the
mini-batch gradient has high variance and taking momentum of a wrong gradient
can't be good. When we put this in the framework of deep learning, it starts to
make sense because there could be multiple local minima (which are all very
good) and saddle points (which you could escape from with SGD naturally). After
sitting through this lecture, **I'm really interested to know more about
optimization and formal theories of deep learning.**

## Sequence to Sequence Networks

This is something that has been really popular in recent times. I've glanced
through some papers on it. I'd really love to try an implementation of this on a
problem.

Oriol and Navdeep did a great job in introducing the various applications and
state-of-the-art methods and for me, what interests me the most would be
attention. **I'd like to learn more about how attention methods
work.**

# Day 2 - Conference


## Session 1 - Deep Generative Models 1

### PixelCNN Models with Auxiliary Variables

Started with an introduction of PixelCNN and sampling in an autoregressive
manner.

They introduce x_hat, an auxiliary variable, that generates x through
p(x|x_hat). These auxiliary variables are images themselves. 4-bit grayscale
images are used for initial experiments.

Pyramid PixelCNN. x_hat is a low resolution view of x, and then generate
recursively. This is simply framing super resolution as image generation.

Nice results on CelebA: generating 128x128 images from 8x8 images.

Results on: https://github.com/kolesman/FaceGeneration

### Parallel Multiscale Autoregressive Density Estimation

PixelRNN and PixelCNN achieve the best natural image density estimation
performance, but sampling is costly: 3N sequential sampling steps for N pixels.

Fast-sampling models still have a big gap in performance (IAF, ConvDraw, NVP,
etc.).

How can we accelerate autoregressive image models with the least performance
degradation?

Main idea of this model is to combine some of these pixels into groups of pixels
and have multiple groups G of such pixels, and all the pixels in the group are
generated in part or at the same time conditioned on the previous group. By
introducing independence assumptions in each group, we can speed up generation
a lot. Every group in the hierarchy uses a different network for prediction. An
improved version adds in local autoregressive dependencies among output pixels.

All these models can be done recursively and super resolution is a natural
extension of this. They generate very realistic looking 8x8 -> 512x512 images!
They have a 100x speed up because it's O(log N)!

So how far can we close this gap? Well, we can't really do it because there's
always conditional dependencies. But we can get better. 

An interesting insight: full sized grid models actually perform worse. 

Conclusion:

- Autoregressive models can be fast to sample from and can generate
high-resolution images and video. 
- Modeling some pixels as conditionally independent is an effecive way to
enable parallelism during sampling
- Multiscale image struture provides a good basis on which to model pixels as
conditionally independent.

### Video Pixel Networks

For learning the distribution of natural videos, we condition not just on the
spatial dependencies but also the previous frames. They have full dependency
across space, time and RGB channels. 

In practice, there's actually blind spots so you have to be aware of that.

See how the lower bound is computed in the paper.

VPN can produce crisp images 10 steps into the future conditioned on 10
previously. 

Examples on nal.ai/vpn

Future work on this would include speeding up and generating beyond 18 frames to
perhaps 100 frames. 

### Learning Texture Manifolds with Periodic Spatial GANs

The talk began with an introduction to texture synthesis. Why is it important?

- The look and feel of a surface is important both in nature and CGI.
- Textures as stochastic processes. Stationary, periodic, ergodic, non-ergodic
  mixing and non-mixing.

Goal: given an example texture image, learn the generating process and sample
textures with the "right" properties. A recent deep parametric approach would be
"Texture Synthetsis Using CNNs" (Gatys et al. 2015). This is basically the style
transfer paper.

All of the current methods have drawbacks:

- Slow to generate large output textures. This is important in many
  applications.
- Cannot handle multiple diverse textures.
- Can't deal with periodic textures. Portilla and Efros can do this in a limited
  way. 
  
They have a previous paper "Texture Synthesis with Spatial GANs" (Jetchev et al.
2016) in NIPS. However, this was limited to stationary, ergodic and mixing. It
can't handle periodic structures.  

Key idea: Combine 3 types of noise tensors Z. Z_local, sampled spatially iid
from noise prior as in SGAN. Z_global, equal on each spatial position.
Z_periodic, periodic filters that look a little like gabor filters and stuff. A
pretty neat idea I must say!

Key abilities:

- learn periodical textures
- learn textures of great variability from large image datasets
- learn whole manifolds of textures and smoothly blend between their elements,
  thus creating novel textures.

Only PSGAN was accurate for the honeycomb. A question I have immediately would
be why is there an input honeycomb? Isn't it supposed to be generating from
noise? I have to read the paper to get the details.

The smooth texture morphing image was pretty cool.

## Session 2 - Deep Generative Models 2

### Generalization and Equilibrium in GANs

http://www.offconvex.org

This talk addresses the following questions:

- Does an equilibrium exist in this 2-person game when Generator has capacity S
  and Discriminator has capacity N?
- If Generator wins, does this mean it has learnt the target distribution (from
  a small number of samples)?

Goodfellow did answer this, if Discriminator capacity, training time, and number
of samples is large (this of course means exponentially large), then yes to
both.

Their theorems:
- Near-equilibrium exists when Generator capacity >= (Discriminator capacity)^2.
  Near in this sense means less than an epsilon.
- But this learnt distribution may be far from target distribution w.r.t any
  standard metric. 
- Only good property they can guarantee is that learnt distribution is
  indistinguishable from target distribution by every Discriminator with the
  stated bound.

They have an empirical contribution of an effective way to add capacity to
generator. **Replace generator by weighted mixture of k generators.** Train
mixing weights via backpropagation; use entropy regularizer to discourage
mixture from collapsing. This often stabilizes and improves training for GANs.
They call it MIX + GAN.

Follow up read: "Do GANs actually learn the distribution? An empirical study."

### McGan: Mean and Covariance Feature Matching GAN

The focus of this paper is on the discriminator.

Known issues in GAN training:

- Loss uncorrelated with sample quality.
- Vanishing gradient needs ad-hoc log(D(G(z)) for G training, so different loss
  for D and G.
- Optimizing a problematic metric (Arjovsky and Bottou 2017).
- Unstable training, mode collapse.

Use Integral Probability Metrics for GAN training. There's lots of theory to
this so I have to read up more on it.

The idea that I got away was there's a target distribution that you're trying to
match to, and you use a neural network to convert the 2 probabilitiy
distributions into some other feature space and measure the distances of these
distributions in the transformed space. 

They have some primal dual form again which I don't understand. And you can
optimize both forms and get similar sample qualities. This is an insanely hard
presentation to understand.

### Learning to Discover Cross-Domain Relations with GANs

The motivation for this paper is that it is hard to collect image pairs. Like
the exact same person but only changing the hair colour or taking off the
glasses. Humans can do it easily (infer the transformation given 2 groups of
images), so can we do it on GANs?

They assume that there exists a transformation in the form of a generator. The
generator takes a blond hair portrait as input and tries to transform it into
a black hair portrait. The discriminator is supposed to distinguish real black
hair from fake black hair. Vanilla GANs in this way does not make sense at all
as the hair might be correct but the face is entirely wrong. This is their
baseline. 

They use another GAN that tries to reconstruct the original image. They coin
this a GAN with reconstruction. 

Their final method is coined DiscoGAN. It's a pretty neat idea because there's a
mirror model. And the GANs have shared weights. Very nice results on gender
conversion. Nice results on handbag to shoe conversion as well.  

Code: https//github.com/SKTBrain/DiscoGAN
Follow up work: Unsupervised Visual Attribute Transfer with Reconfigurable GAN.

Questions from audience:

- How is this different from CycleGAN from UC Berkeley? CycleGAN is a parallel
  line of work. https://arxiv.org/abs/1703.10593.

### Conditional Image Synthesis with Auxiliary Classifier GANs

Poor flow and structure to overall presentation. It was really hard to follow.
I'd read the paper if I want to know more. 

### Wasserstein GAN

The paper that I'd really like to hear about! I see lots of post online on how
WGAN really improves GAN.

TLDR: Check out Reddit for the nice TLDR.

- As the discriminator gets better, updates to the generator get worse. 
- Discriminator achieves 1 test accuracy quickly.
- Real data is usually concentrated in a low dimensional manifold.

Basically, there are problems with JSD and KL. Wasserstein distance is a
"better" metric for measuring the distance between the 2 distributions you're
trying to match. 

It looks tough at the start, but thankfully, there's a dual. 

Idea: train one net f (critic) to maximize the dual then do gradient descent on
theta. 

To make 1-Lipschitz or K-Lipschitz, clip weights! It's a terrible idea but it
kind of works. There is also follow up work for this. 

Conclusion:
- Use difference instead of CE, not sigmoid or logs.
- Enforce a Lipschitz constraint (clipping or gradient penalty).
- Train disc/critic to optimality (ncritic=5).

## Session 3 - Deep Generative Models 3

### Learning Hierarchical Features from Generative Models

Hierarchical injection of randomness. As we go up the feature hierarchy, it
becomes increasingly abstract.

Limitations of Stacked Hierarchical Generative Models.
- Bottlenecked by bottom layer, higher layers are redundant.
- Stacked HGM with simple conditional do not learn hierarchical features. 

The basic assumption is that more complex abstract features require deeper
networks to model. They coin it architectural hierarchy. Their model is
Variational Ladder Autoencoder (VLAE). It's different from LVAE. This paper
gives a significant step forward on the ability to disentangle the latent codes.

### Bottleneck Conditional Density Estimation

Main motivation of their work is conditional density estimation. Take the top
half of the face and try to impute the bottom half (basically inpainting). Very
mathematically heavy presentation and there's a need to read the paper to get
the details. 

### Learning Deep Latent Gaussian Models with Markov Chain Monte Carlo

Very mathematically heavy and there's a need to read the paper to get the
details.

## Session 3 - Deep Learning Theory 3

### Depth-Width Tradeoffs with Neural Networks

Joined this talk towards the end and saw many mathematical equations. I doubt I
would have understood it even if I sat in from the start. Interesting title
though.

## Session 4 - Deep Learning 4

### Learning to learn without gradient descent by gradient descent

Goal of this paper is to have data efficient black-box optimization. Let the RNN
do optimization.

Started off with a background on Bayesian Optimization. Model based sequential
optimization algorithm. They use some posterior and expected improvement.
Traditionally very difficult or slow to compute. I don't know much about it and
need to read more. 

Excellent results overall. Competitive convergence properties and speed compared
to existing methods.

### Learned Optimizers that Scale and Generalize

Optimization requires hand tuning and manual supervision. Can optimizers be
learned that work well on a variety of problems? Previous work can outperform
existing optimizers but generalize and scale poorly. They introduce hierarchical
RNNs.

In essence, it's a black box that takes in parameters and gradients and tries to
generate a parameter update. Generally, history is important for this and thus
RNNs are a natural choice. 

Per-parameter RNN has some advantages and disadvantages. A key disadvantage is
that it fails to generalize (for some reason). Their contributions include
architecture, design features, and the training process. 

In terms of architecture, they still have a per-parameter RNN. They now pipe
this into a block and have a Tensor RNN. Every layer has a different Tensor RNN
that can coordinate information within each parameter. They then add another
Global RNN. Natural idea of parameter sharing.

In terms of design features, they incorporate design features from optimization
literature, like momentum and dynamic input scaling (similar to RMSProp and
ADAM). 

They use a meta-training ensemble to train this black box. Convex and non-convex
objectives, stochastic and deterministic, problems with poor scaling, no neural
networks in the training set! Just small toy problems. 

### Learning Gradient Descent: Better Generalization and Longer Horizons

Contributions: 
- Random scaling.
- Combination with Convex Functions.
- RNNprop. Normalized gradient for RMSprop and normalized momentum in ADAM.
  They propose to feed some squared rooted value in instead.

### Learning Algorithms for Active Learning

Active learning. Goal is to train a model that makes useful predictions on new
data. But in the active learning settings, there may be a few or no labels
available for training. Gathering labels may be costly too. The solution is to
collect labels for a subset of points - balance labelling costs and prediction
quality. Some strategies include updating classifier online, then label an
instance that confuses the classifier. Bayesian approach is to label the
instance expected to provide the most information.

An example is movie recommendations. With a cold start, how do we ask a new
user? Their approach is to use other users' ratings to learn a strategy for
selecting informative movies. 

Key motivation for their paper is that many methods have been developed for
active learning but make strong assumptions and require approximations. Through
metalearning, they can learn an active learning algorithm end-to-end. They train
a model to do active learning on data from related tasks. The learned strategy
should apply to new tasks from the same distribution. This allows co-adaptation.

Related papers include "Active One-Short Learning" (Woodward and Finn, 2016) and
Matching Network (Vinyals et al. 2016).

In summary, their model learns to actively build the labeled support set for a
Matching Network. 

